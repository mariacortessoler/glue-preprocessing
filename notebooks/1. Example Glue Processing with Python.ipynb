{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue Job Preprocessing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Python shell AWS Glue job, you can run scripts that are compatible with Python 2.7 or Python 3.6. This is an excellent option for processing small or medium datasets. In this notebook we execute a Python script in a Glue job using the AWS Wrangler library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents' />\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Loading libraries](#loading)\n",
    "2. [Creating S3 bucket](#s3bucket)\n",
    "3. [Packing needed libraries](#libraries)\n",
    "4. [Creating our AWS Glue ETL Python script](#etl)\n",
    "5. [Create an IAM role for executing the AWS Glue job](#iam)\n",
    "6. [Creating our Glue job](#job)\n",
    "7. [Running our Glue job](#run)\n",
    "8. [Review the transformed data](#review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loading' />\n",
    "\n",
    "## 1. Loading libraries:\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the packages included in the `requirements.txt`file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "lakeformation = boto3.client('lakeformation')\n",
    "iam = boto3.client('iam')\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('../data/type2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='s3bucket' />\n",
    "\n",
    "## 2.  Creating a S3 bucket for storing data and AWS Glue required artifacts\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a bucket in S3 to store our scripts, libraries, data and results from our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket_name='preprocessing-example' #Fill your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: preprocessing-example\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb s3://{data_bucket_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data files in the S3 bucket you just created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Events file: This file should be saved in the following path, for the Glue jobs to run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./events.parquet to s3://preprocessing-example/data/raw/events/dt=2020-06-30/events.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ../data/events.parquet s3://{data_bucket_name}/data/raw/events/dt=2020-06-30/events.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Users file: This file should be saved in the following path, for the Glue jobs to run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./users.parquet to s3://preprocessing-example/data/raw/users/dt=2020-06-30/users.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ../data/users.parquet s3://{data_bucket_name}/data/raw/users/dt=2020-06-30/users.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transactions files: Transactions files should be saved in the following paths, for the Glue jobs to run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./type1.parquet to s3://preprocessing-example/data/raw/transactions/type_1/dt=2020-06-30/type1.parquet\n",
      "upload: ./type2.parquet to s3://preprocessing-example/data/raw/transactions/type_2/dt=2020-06-30/type2.parquet\n"
     ]
    }
   ],
   "source": [
    "#Type 1 transactions\n",
    "!aws s3 cp ../data/type1.parquet s3://{data_bucket_name}/data/raw/transactions/type_1/dt=2020-06-30/type1.parquet\n",
    "#Type 2 transactions\n",
    "!aws s3 cp ../data/type2.parquet s3://{data_bucket_name}/data/raw/transactions/type_2/dt=2020-06-30/type2.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Labels file: This file should be saved in the following path, for the Glue jobs to run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./labels.csv to s3://preprocessing-example/labels/dt=2020-06-30/labels.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ../data/labels.csv s3://{data_bucket_name}/labels/dt=2020-06-30/labels.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries' />\n",
    "\n",
    "## 3.  Packing needed libraries\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment for running a Python shell job supports the following libraries:\n",
    "\n",
    "```\n",
    "Boto3\n",
    "collections\n",
    "CSV\n",
    "gzip\n",
    "multiprocessing\n",
    "NumPy\n",
    "pandas\n",
    "pickle\n",
    "PyGreSQL\n",
    "re\n",
    "SciPy\n",
    "sklearn\n",
    "sklearn.feature_extraction\n",
    "sklearn.preprocessing\n",
    "xml.etree.ElementTree\n",
    "zipfile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to these libraries, in this job we will use the `awswrangler` library. For this, we will create a Python Wheels package with this library. For this, we will create a `setup.py` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name=\"glue_monthly_stage_dependencies\", #Name for your Python Wheels package\n",
    "    version=\"0.1\",\n",
    "    install_requires=[\n",
    "        \"awswrangler==2.4.0\" #Required library and version\n",
    "    ]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following command, that will create a `dist` folder and a `glue_monthly_stage_dependencies-0.1-py3-none-any.whl` file within. This is `whl` file that we will use for using `awswrangler` library in our Glue job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "installing to build/bdist.linux-x86_64/wheel\n",
      "running install\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating glue_monthly_stage_dependencies.egg-info\n",
      "writing glue_monthly_stage_dependencies.egg-info/PKG-INFO\n",
      "writing dependency_links to glue_monthly_stage_dependencies.egg-info/dependency_links.txt\n",
      "writing requirements to glue_monthly_stage_dependencies.egg-info/requires.txt\n",
      "writing top-level names to glue_monthly_stage_dependencies.egg-info/top_level.txt\n",
      "writing manifest file 'glue_monthly_stage_dependencies.egg-info/SOURCES.txt'\n",
      "reading manifest file 'glue_monthly_stage_dependencies.egg-info/SOURCES.txt'\n",
      "writing manifest file 'glue_monthly_stage_dependencies.egg-info/SOURCES.txt'\n",
      "Copying glue_monthly_stage_dependencies.egg-info to build/bdist.linux-x86_64/wheel/glue_monthly_stage_dependencies-0.1-py3.6.egg-info\n",
      "running install_scripts\n",
      "creating build/bdist.linux-x86_64/wheel/glue_monthly_stage_dependencies-0.1.dist-info/WHEEL\n",
      "creating 'dist/glue_monthly_stage_dependencies-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "adding 'glue_monthly_stage_dependencies-0.1.dist-info/METADATA'\n",
      "adding 'glue_monthly_stage_dependencies-0.1.dist-info/WHEEL'\n",
      "adding 'glue_monthly_stage_dependencies-0.1.dist-info/top_level.txt'\n",
      "adding 'glue_monthly_stage_dependencies-0.1.dist-info/RECORD'\n",
      "removing build/bdist.linux-x86_64/wheel\n"
     ]
    }
   ],
   "source": [
    "!python3 setup.py bdist_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='etl' />\n",
    "\n",
    "## 4.  Creating our AWS Glue ETL Python script\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the ETL script for the data preprocessing. We will use the following parameters:\n",
    "\n",
    "- `today`: Date for executing the job in format: `yyyy-mm-dd`. The value used for this example is `2020-07-01`\n",
    "- `data_bucket_name`: Name of the bucket you created for storing data and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/glue_pyhton_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/glue_pyhton_script.py\n",
    "\n",
    "print('Loading the required Python libraries')\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from awsglue.utils import getResolvedOptions\n",
    "import gc\n",
    "import boto3\n",
    "import sys\n",
    "\n",
    "print('Reading the AWS Glue job parameters')\n",
    "args = getResolvedOptions(sys.argv,['today', 'data_bucket_name'])\n",
    "today = datetime.strptime(args['today'], '%Y-%m-%d').date()\n",
    "data_bucket_name = args['data_bucket_name']\n",
    "first_day_current_month=today.replace(day=1)\n",
    "last_day_previous_month=(first_day_current_month - timedelta(days=1))\n",
    "\n",
    "print('Reading the two transactions types data')\n",
    "transactions_type1=wr.s3.read_parquet(path=f's3://{data_bucket_name}/data/raw/transactions/type_1/dt={last_day_previous_month}')\n",
    "transactions_type2=wr.s3.read_parquet(path=f's3://{data_bucket_name}/data/raw/transactions/type_2/dt={last_day_previous_month}')\n",
    "\n",
    "print('Merging the two transactions types data')\n",
    "accounts_=pd.Series(list(transactions_type1.id.unique())+list(transactions_type2.id.unique())).unique().astype('Int64')\n",
    "df_transactions=( \n",
    "    pd.DataFrame({'id':list(accounts_)})\n",
    "    .merge(transactions_type1,how='left',on='id')\n",
    "    .merge(transactions_type2,how='left',on='id')\n",
    ")\n",
    "\n",
    "print('Delete temporal datasets no longer required')\n",
    "del transactions_type1, transactions_type2, accounts_\n",
    "gc.collect()\n",
    "df_transactions.dropna(subset=['id'], inplace=True)\n",
    "df_transactions['id']=pd.to_numeric(df_transactions['id'], errors='coerce').astype(np.int64)\n",
    "df_transactions.fillna(0, inplace=True)\n",
    "\n",
    "print('Read events data')\n",
    "df_events=wr.s3.read_parquet(path=f's3://{data_bucket_name}/data/raw/events/dt={last_day_previous_month}')\n",
    "\n",
    "print('Merging users and events data')\n",
    "df_final=pd.merge(df_transactions, df_events, how='left', left_on='id', right_on='id')\n",
    "del df_events\n",
    "gc.collect()\n",
    "#Fill missing values\n",
    "df_final['fl_os_android']=df_final['fl_os_android'].fillna(1)\n",
    "df_final['fl_os_ios']=df_final['fl_os_ios'].fillna(0)\n",
    "df_final.fillna(0, inplace=True)\n",
    "df_final['dt']=str(last_day_previous_month)\n",
    "\n",
    "print('Save the final dataset in S3')\n",
    "wr.s3.to_parquet(df_final,\n",
    "                 path=f's3://{data_bucket_name}/data/monthly_stage/',\n",
    "                 dataset=True,\n",
    "                 partition_cols=['dt'],\n",
    "                 mode=\"overwrite_partitions\",\n",
    "                 concurrent_partitioning=True,\n",
    "                 index=False\n",
    "                )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the AWS Glue script and libraries file on S3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will the Python script `glue_monthly_stage.py` and the Python Dependencies `whl` file `glue_monthly_stage_dependencies-0.1-py3-none-any.whl` in the S3 bucket we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving glue_monthly_stage.py file\n",
    "s3.meta.client.upload_file('../src/glue_pyhton_script.py', #Name of the Python script\n",
    "                            data_bucket_name, #Bucket name for saving artifacts\n",
    "                           'artifacts/code/monthly_stage/glue_pyhton_script.py' #Include the key and filename\n",
    "                          )\n",
    "# Saving the whl dependencies file\n",
    "s3.meta.client.upload_file('dist/glue_monthly_stage_dependencies-0.1-py3-none-any.whl', #Whl dependencies file\n",
    "                           data_bucket_name, #Bucket name for saving artifacts\n",
    "                           'artifacts/code/monthly_stage/glue_monthly_stage_dependencies-0.1-py3-none-any.whl' #Include the key and filename\n",
    "                          ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iam' />\n",
    "\n",
    "## 5.  Create an IAM role for executing the AWS Glue job\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create an IAM role with AWSGlueServiceRole and AmazonS3FullAccess managed policies, for the data preprocessing Glue job. For doing this, you need to execute the following cells with an IAM role that has permissions for creating roles and attaching policies, as shown below:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:CreateRole\",\n",
    "                \"iam:AttachRolePolicy\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = 'GluePreprocessingRole' #Specify the role name to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating an AWS Glue role with AWSGlueServiceRole and AmazonS3FullAccess policies\n",
    "def create_glue_role(role_name):\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "              \"Effect\": \"Allow\",\n",
    "              \"Principal\": {\n",
    "                \"Service\": \"glue.amazonaws.com\"\n",
    "              },\n",
    "              \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print('Creating role: {}...'.format(role_name))\n",
    "        create_role_response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Role creation failed. Likely already existed.')\n",
    "\n",
    "    print('Attaching Glue Service Role policy')\n",
    "    glue_policy_arn = 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'\n",
    "    iam.attach_role_policy(\n",
    "        RoleName  = role_name,\n",
    "        PolicyArn = glue_policy_arn\n",
    "    )\n",
    "    print('Attaching S3 full access policy...')\n",
    "    s3_policy_arn = 'arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    "    iam.attach_role_policy(\n",
    "        RoleName  = role_name,\n",
    "        PolicyArn = s3_policy_arn\n",
    "    )\n",
    "\n",
    "    print('Waiting for policy attachment to propagate...')\n",
    "    time.sleep(60) # Wait for a minute to allow IAM role policy attachment to propagate\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating role: GluePreprocessingRole...\n",
      "Attaching Glue Service Role policy\n",
      "Attaching S3 full access policy...\n",
      "Waiting for policy attachment to propagate...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_glue_role(role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='job' />\n",
    "\n",
    "## 6.  Creating our Glue Job\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set for creating our Glue job and starting a run. for this, we must provide the following properties:\n",
    "\n",
    "- **IAM role:** Specify the IAM role created before.\n",
    "\n",
    "- **Type:** Choose Python shell to run a Python script with the job command named pythonshell.\n",
    "\n",
    "- **Python version:** Choose the Python version. The default is Python 3.\n",
    "\n",
    "- **Custom script:** You must provide the script location in Amazon S3. \n",
    "\n",
    "- **Maximum capacity:** The maximum number of AWS Glue DPUs that can be allocated when the job runs. A DPU is a relative measure of processing power that consists of 4 vCPUs of compute capacity and 16 GB of memory. For Python Shell jobs, you can set the value to 0.0625 or 1. The default is 0.0625.\n",
    "\n",
    "- **Extra-py-files (Python library path):** The Amazon S3 location of one or more Python libraries packaged as an `.egg` or a `.whl` file. In this example we created a `.whl` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name='job_aiml_monthly_stage_preprocess' #Replace with the name for your Glue job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Glue job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = glue.create_job(Name=job_name, \n",
    "                      Role=role_name,\n",
    "                      Command={'Name': 'pythonshell', #We specify that we will create a Python Shell job\n",
    "                               \"PythonVersion\" : \"3\",\n",
    "                               'ScriptLocation': f's3://{data_bucket_name}/artifacts/code/monthly_stage/glue_pyhton_script.py'}, #S3 location of the python script\n",
    "                      DefaultArguments={\n",
    "                        '--extra-py-files': f's3://{data_bucket_name}/artifacts/code/monthly_stage/glue_monthly_stage_dependencies-0.1-py3-none-any.whl'}, #S3 location for dependencies file\n",
    "                      MaxCapacity=1 #This is the maximum capacity for Python Shell jobs\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run' />\n",
    "\n",
    "## 7.  Running our Glue Job\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arguments needed for Glue job run:\n",
    "today = '2020-07-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "        '--today':today,\n",
    "        '--data_bucket_name': data_bucket_name,\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the job to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WAIT_TIME=time.time() + 60*10 # Maximum wait time set to 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review' />\n",
    "\n",
    "## 8.  Review the transformed data\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Review the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.strptime(today, '%Y-%m-%d').date()\n",
    "first_day_current_month=today.replace(day=1)\n",
    "last_day_previous_month=(first_day_current_month - timedelta(days=1))\n",
    "df=wr.s3.read_parquet(path=f's3://{data_bucket_name}/data/monthly_stage/dt={str(last_day_previous_month)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
