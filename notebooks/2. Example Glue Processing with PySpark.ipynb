{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Glue Processing with PySpark and additional Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Glue supports an extension of the PySpark Python dialect for scripting extract, transform, and load (ETL) jobs. In this notebook we use this dialect for creating an ETL script to run a Glue job. Moreover, we use the AWS Glue Version 2.0 that allows to provide additional Python modules. In this case, we use the `dateutil` Python extension module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents' />\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Loading libraries](#loading)\n",
    "2. [Creating our AWS Glue ETL PySpark script](#etl)\n",
    "6. [Creating our Glue job](#job)\n",
    "7. [Running our Glue job](#run)\n",
    "8. [Review the transformed data](#review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loading' />\n",
    "\n",
    "## 1. Loading libraries:\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import awswrangler as wr\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "ssm = boto3.client('ssm') \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required parameters:\n",
    "data_bucket_name='<YOUR-BUCKET-NAME>' #Replace with the name of the bucket you created in the previous Jupyter Notebook\n",
    "role_name = 'GluePreprocessingRole' #Specify the role name to create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='etl' />\n",
    "\n",
    "## 2. Creating our AWS Glue ETL PySpark script\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the ETL script for the data prepration. We will use the following parameters:\n",
    "\n",
    "- `today`: Date for executing the job in format: `yyyy-mm-dd`. The value used for this example is `2020-07-01`\n",
    "- `data_bucket_name`: Name of the bucket you created for storing data and scripts.\n",
    "- `months`: Parameter to filter the time since creation of users. We will use `6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/glue_pyspark_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/glue_pyspark_script.py\n",
    "\n",
    "print('Loading the required libraries')\n",
    "import sys\n",
    "import pyspark.sql.functions as func\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "import boto3\n",
    "import ast\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "import sys\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "print('Reading the AWS Glue job parameters')\n",
    "args = getResolvedOptions(sys.argv,['today', 'data_bucket_name', 'months'])\n",
    "today = datetime.strptime(args['today'], '%Y-%m-%d').date()\n",
    "months = int(args['months'])\n",
    "data_bucket_name = args['data_bucket_name']\n",
    "first_day_current_month=today.replace(day=1)\n",
    "last_day_previous_month=(first_day_current_month + relativedelta(days=-1))\n",
    "print('Last day previous month: ', last_day_previous_month)\n",
    "\n",
    "print('Set Spark configurations')\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "print('Declaring required functions')\n",
    "#The retrieve_files \n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    \"\"\"This is a function that returns a list of S3 objects that: a) are located within the path parameter,\n",
    "    b) have the same file type as the parameter, and c) are located in the date partitions from the list_dates\n",
    "    parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    path : str\n",
    "        The S3 path. Retrieved files must be located within this path.\n",
    "    file_type : str\n",
    "        The file type. Retrieved files must have this file type.\n",
    "    list_dates : list\n",
    "        List of dates. Retrived files must be in a date partition that belongs to this list.\n",
    "        \"\"\"\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "print('Reading users relevant data')\n",
    "users_file=retrieve_files(path=f's3://{data_bucket_name}/data/raw/users/', \n",
    "                          file_type='parquet', list_dates=[str(last_day_previous_month)])[0]\n",
    "print('Users file: {}'.format(users_file))\n",
    "df_users = spark.read.option(\"header\",\"true\").parquet(users_file).select('id', 'created_date', 'fl_sexo_M',\n",
    "       'fl_ubicacion_centro', 'vl_edad',\n",
    "       'fl_tarjeta_chip', 'fl_tarjeta_normal',\n",
    "       'fl_bancarizado', 'nu_bcra_entidades', 'max_situacion',\n",
    "       'max_dias_atraso', 'fl_opera_banco')\n",
    "\n",
    "print('Making required transformations to users dataset')\n",
    "df_users = df_users.select('*',col('id').cast(LongType()).alias('id2'))\n",
    "df_users = df_users.drop(col('id')).withColumnRenamed('id2', 'id')\n",
    "df_users = df_users.withColumn('vl_antiguedad_dias', datediff(to_date(lit(str(first_day_current_month)), 'yyyy-MM-dd'),df_users.created_date))\n",
    "df_users = df_users.select(\"*\",to_date(col('created_date').cast(StringType()), 'yyyy-MM-dd').alias('created_date2'))\n",
    "df_users = df_users.drop(col('created_date')).withColumnRenamed('created_date2', 'created_date')\n",
    "del users_file\n",
    "gc.collect()\n",
    "\n",
    "print('Filter users by their creation date')\n",
    "first_date=(first_day_current_month+relativedelta(months=-months)-timedelta(days=1))\n",
    "print('First date of history: {}'.format(first_date))\n",
    "df_users2 = df_users.filter(col('created_date') > lit(str(first_date)))\n",
    "del df_users\n",
    "gc.collect()\n",
    "\n",
    "print('Reading churn labels for model')\n",
    "labels_file=retrieve_files(path=f's3://{data_bucket_name}/labels/', file_type='csv', list_dates=[str(last_day_previous_month)])[0]\n",
    "print('Labels file: {}'.format(labels_file))\n",
    "\n",
    "print('Read the real churn labels for users')\n",
    "df_labels = spark.read.option(\"header\",\"true\").csv(labels_file).select('id', 'churn_label')\n",
    "df_labels = df_labels.select('*', col('id').cast(LongType()).alias('id2'))\n",
    "df_labels = df_labels.drop(col('id')).withColumnRenamed('id2', 'id')\n",
    "del labels_file\n",
    "gc.collect()\n",
    "\n",
    "print('Join users dataset with labels')\n",
    "df_users3 = df_users2.join(df_labels, on=['id'], how = 'inner')\n",
    "del df_users2, df_labels\n",
    "gc.collect()\n",
    "\n",
    "print('Reading the monthly_stage file')\n",
    "monthly_stage_file=retrieve_files(f's3://{data_bucket_name}/data/monthly_stage/', 'parquet', list_dates=[str(last_day_previous_month)])[0]\n",
    "print('Monthly stage file: {}'.format(monthly_stage_file))\n",
    "df_total=spark.read.option(\"header\",\"true\").parquet(monthly_stage_file)\n",
    "del monthly_stage_file\n",
    "gc.collect()\n",
    "\n",
    "# Making required transformations\n",
    "df_total= df_total.select('*', col('id').cast(LongType()).alias('id2'))\n",
    "df_total = df_total.drop(col('id')).withColumnRenamed('id2', 'id')\n",
    "print('Join monthly stage file with users')\n",
    "df_total2 = df_total.join(df_users3, on=['id'], how = 'inner')\n",
    "del df_total\n",
    "gc.collect()\n",
    "\n",
    "print('Making required transfromations')\n",
    "print('Variables to aggregate')\n",
    "sum_variables_events=['nu_bloqueo', 'nu_incidente','nu_inicio']\n",
    "last_variables_events=['fl_os_android',\n",
    "                       'fl_carrier_1','fl_carrier_2','fl_carrier_3','fl_carrier_4']\n",
    "\n",
    "sum_variables_transactions=[i for i in df_total2.columns \n",
    "                           if i[:3] in ['vl_','nu_'] and \n",
    "                              i not in sum_variables_events+\n",
    "                                       last_variables_events+\n",
    "                                      ['vl_edad', 'vl_antiguedad_dias', 'nu_bcra_entidades']] \n",
    "\n",
    "print('Sum variables')\n",
    "df_total2=df_total2.select('*', col('id').cast(LongType()).alias('id2'))\n",
    "df_total2=df_total2.drop(col('id')).withColumnRenamed('id2', 'id')\n",
    "sum_variables=sum_variables_transactions+sum_variables_events\n",
    "sum_exprs = [sum(x).alias('{0}'.format(x)) for x in sum_variables]\n",
    "df_base=df_total2.groupBy('id').agg(*sum_exprs)\n",
    "df_base=df_base.select('*', col('id').cast(LongType()).alias('id2'))\n",
    "df_base=df_base.drop(col('id')).withColumnRenamed('id2', 'id')\n",
    "\n",
    "print('Take the last value for certain variables')\n",
    "last_exprs = [last(x).alias('{0}'.format(x)) for x in last_variables_events]\n",
    "df_base1=df_base.join(df_total2.groupBy('id').agg(*last_exprs),on='id',how='inner')\n",
    "del df_base, last_exprs, sum_exprs, sum_variables, sum_variables_events, sum_variables_transactions\n",
    "gc.collect()\n",
    "df_base2=df_base1.join(df_users3, how='inner', on='id')\n",
    "del df_users3, df_total2, df_base1\n",
    "gc.collect()\n",
    "\n",
    "print('Cast numeric variables')\n",
    "numeric_columns = [f.name for f in df_base2.schema.fields if (isinstance(f.dataType, (DoubleType, IntegerType, LongType, ShortType, FloatType,DecimalType)))]\n",
    "numeric_columns=[i for i in numeric_columns if i not in ['id']]\n",
    "integer_columns=[i for i in numeric_columns if ((i.startswith(('fl_', 'nu_', 'cd_', 'max_')) )| (any(x in i.lower() for x in ['vl_edad','vl_antiguedad_dias'])))]\n",
    "double_columns=[i for i in numeric_columns if i not in integer_columns]\n",
    "for i in integer_columns:\n",
    "    df_base2=df_base2.withColumn(i, col(i).cast(IntegerType()))\n",
    "for d in double_columns:\n",
    "    df_base2=df_base2.withColumn(d, col(d).cast(DoubleType()))\n",
    "\n",
    "print('Write the data in S3 with parquet format')\n",
    "df_base2.write\\\n",
    "    .format('parquet')\\\n",
    "    .save(f's3://{data_bucket_name}/data/processed/processed_dt={str(last_day_previous_month)}', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the AWS Glue script on S3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will save the Python script `glue_pyspark_script.py` in the S3 bucket we created in the previous Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving glue_processed_pyspark.py file in S3\n",
    "s3.meta.client.upload_file('../src/glue_pyspark_script.py', #Name of the Python script\n",
    "                            data_bucket_name, #Bucket name for saving artifacts\n",
    "                           'artifacts/code/processed/glue_pyspark_script.py' #Include the key and filename\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='job' />\n",
    "\n",
    "## 3.  Creating our Glue Job\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of jobs in AWS Glue: Spark, Streaming ETL, and Python shell.\n",
    "\n",
    "In this case, we will use a Spark job that is executed in an Apache Spark environment managed by AWS Glue. For this, we will specify the following property:\n",
    "\n",
    "- **Type:** This property specifies the type of job environment to run. We choose Spark to run an Apache Spark ETL script with the job command `glueetl`.\n",
    "\n",
    "- **AWS Glue version:** Determines the versions of Apache Spark and Python that are available to the job. We choose version `2.0`, that supports Spark `2.4.3` and Python `3.7`. \n",
    "\n",
    "- **Worker type**: We choose the `G.1X` worker type, recommended for memory-intensive jobs. This is the default Worker type for AWS Glue Version 2.0 jobs. Each worker maps to 1 DPU (4 vCPU, 16 GB of memory, 64 GB disk)\n",
    "\n",
    "- **Number of workers**: We will specify 2. The maximum number of workers you can define is 299 for G.1X.\n",
    "- **Additional Python modules**: AWS Glue Version 2.0 also allows providing additional Python modules, with a list of comma-separated Python modules. In this case, we will add `dateutil==2.8.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name='job_aiml_processed' #Replace with the name for your Glue job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Glue job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete job if it already exists\n",
    "glue.delete_job(JobName=job_name)\n",
    "#Create the glue job\n",
    "job = glue.create_job(Name=job_name, \n",
    "                      GlueVersion='2.0',\n",
    "                      Role=role_name,\n",
    "                      Command={'Name': 'glueetl',\n",
    "                               'ScriptLocation': f's3://{data_bucket_name}/artifacts/code/processed/glue_pyspark_script.py'},\n",
    "                      DefaultArguments={\n",
    "                        '--additional-python-modules': 'dateutil==2.8.1' #You can add any additional python modules here\n",
    "                      },\n",
    "                      WorkerType='G.1X',\n",
    "                      NumberOfWorkers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run' />\n",
    "\n",
    "## 4.  Running our Glue Job\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arguments needed for Glue job run:\n",
    "today = '2020-07-01' #Date for executing the job in format: yyyy-mm-dd\n",
    "months=str(6) #Parameter to filter the time since creation of users. We will use 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "        '--today':today,\n",
    "        '--data_bucket_name': data_bucket_name,\n",
    "        '--months': months\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the job to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_WAIT_TIME=time.time() + 60*10 # 1 hour of maximum wait time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review' />\n",
    "\n",
    "## 5.  Review the transformed data\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.strptime(today, '%Y-%m-%d').date()\n",
    "first_day_current_month=today.replace(day=1)\n",
    "last_day_previous_month=(first_day_current_month + relativedelta(days=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=wr.s3.read_parquet(f's3://{data_bucket_name}/data/processed/processed_dt={str(last_day_previous_month)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review' />\n",
    "\n",
    "## 6.  Clean up resources\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished executing our Glue jobs, we will delete all the files that we created in our S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm s3://{data_bucket_name} --recursive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
